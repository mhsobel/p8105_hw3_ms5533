Homework 3
================
Marisa Sobel
10/7/2018

## Problem 1

Data
cleaning

``` r
# import brfss_smart2010 data, clean names, filter by "Overall Health", remove unwanted vars
# make "response" a factor variable
brfss_data = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-class, -topic, -question, 
         -sample_size, -(confidence_limit_low:geo_location)) %>% 
  mutate(
    response = factor(response, 
                      levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
  )
```

### Questions

1.  *_In 2002, which states were observed at 7
locations?_*

<!-- end list -->

``` r
# Filter by year, group by state, summarize number of locations in each state
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarise(
    number = n_distinct(locationdesc)
  ) %>% 
  filter(number == 7)
## # A tibble: 3 x 2
##   locationabbr number
##   <chr>         <int>
## 1 CT                7
## 2 FL                7
## 3 NC                7
```

**Connecticut, Florida, and North Caroline** were observed at 7
locations in 2002.

2.  *_Make a “spaghetti plot” that shows the number of observations in
    each state from 2002 to
2010._*

<!-- end list -->

``` r
# Group by year and state, summarize number of locations in each state (n)
# Line plot of year vs n, turn off legend --> too many states
brfss_data %>% 
  group_by(year, locationabbr) %>% 
  summarise(
    number = n()) %>% 
  ggplot(aes(x = year, y = number, color = locationabbr)) +
  geom_line() +
   labs(
    title = "Number of Observations by State (2002-2010)", 
    x = "Year", 
    y = "Number of Obs", 
    legend = "State") +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme(legend.position = "none")
```

![](p8105_hw3_ms5533_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

3.  *_Make a table showing, for the years 2002, 2006, and 2010, the mean
    and standard deviation of the proportion of “Excellent” responses
    across locations in NY State._*

<!-- end list -->

``` r
# Filter: 1) NY, 2) year (2002, 2006, 2010)
# Group by location 
# "data_value" = proportion 
excellent_table = 
brfss_data %>% 
  filter(locationabbr == "NY") %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  filter(response == "Excellent") %>% 
  group_by(year) %>% 
  summarise(
    mean = mean(data_value),
    std_dev = sd(data_value))
```

**Average and standard deviation of proportion of “Excellent” responses
in NY**

| year |  mean | std\_dev |
| ---: | ----: | -------: |
| 2002 | 24.04 |     4.49 |
| 2006 | 22.53 |     4.00 |
| 2010 | 22.70 |     3.57 |

4.  *_For each year and state, compute the average proportion in each
    response category (taking the average across locations in a state).
    Make a five-panel plot that shows, for each response category
    separately, the distribution of these state-level averages over
    time._*

<!-- end list -->

``` r
# Group by year, state, and response, mean of "data_value"
# Line plot year vs mean, facet by "response", turn off legend --> too many states 
brfss_data %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(
    mean = mean(data_value, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = year, y = mean, color = locationabbr)) +
  geom_line(alpha = .5) +
  facet_grid(~ response) + 
  labs(
    title = "Average Percent of Each Response by State (2002-2010)", 
    x = "Year", 
    y = "Average Percent", 
    legend = "State") +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme(legend.position = "none")
```

![](p8105_hw3_ms5533_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

``` r

# DONT LOVE THE X AXIS ON THIS ONE
```

## Problem 2

Data exploration

``` r
# import dataset
instacart_data = instacart 

instacart_data %>% 
  summarise(
    sum(reordered)/nrow(instacart_data)*100
  )
## # A tibble: 1 x 1
##   `sum(reordered)/nrow(instacart_data) * 100`
##                                         <dbl>
## 1                                        59.9
instacart_data %>% 
  summarise(mean(order_number))
## # A tibble: 1 x 1
##   `mean(order_number)`
##                  <dbl>
## 1                 17.1
```

In the present Instacart dataset, there are **1384617** observations of
specific items ordered with information across **15** variables.
**59.9%** of items were reorded from previous orders. The listed order
was, on average, the **17th** order for individuals.

``` r
# trying to make plots...not working

hour_of_day_plot = 
  instacart_data %>% 
  ggplot(aes(x = order_hour_of_day)) +
  geom_histogram()

days_since_prior_plot

dow_plot

department_plot

# (tmax_tmin_p + prcp_dens_p) / tmax_date_p
```

### Questions

1.  *_How many aisles are there, and which aisles are the most items
    ordered from?_*

<!-- end list -->

``` r
# Distinct aisles
instacart_data %>% 
  summarise(number = n_distinct(aisle_id))
## # A tibble: 1 x 1
##   number
##    <int>
## 1    134

# Number of items in each aisle --> arrange desc
instacart_data %>%
  group_by(aisle) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number))
## # A tibble: 134 x 2
##    aisle                         number
##    <chr>                          <int>
##  1 fresh vegetables              150609
##  2 fresh fruits                  150473
##  3 packaged vegetables fruits     78493
##  4 yogurt                         55240
##  5 packaged cheese                41699
##  6 water seltzer sparkling water  36617
##  7 milk                           32644
##  8 chips pretzels                 31269
##  9 soy lactosefree                26240
## 10 bread                          23635
## # ... with 124 more rows
```

There are **134** distinct aisles. **Fresh vegetables** and **frush
fruits** are the two most frequenly ordered from aisles by a factor of
10, followed by **package vegetables/fruits, yogurt, and packaged
cheese**.

2.  *_Make a plot that shows the number of items ordered in each aisle.
    Order aisles sensibly, and organize your plot so others can read
    it._*

<!-- end list -->

``` r
# THESE ARE LOOKING REAL FUNKY...

instacart_data %>%
  group_by(aisle, department) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number)) %>% 
  ggplot(aes(x = aisle, y = number)) +
  geom_bar(stat = "identity") +
  facet_grid(~ department)
```

3.  *_Make a table showing the most popular item aisles “baking
    ingredients”, “dog food care”, and “packaged vegetables fruits”._*

<!-- end list -->

``` r
# NOT SURE WHAT THIS IS ASKING...

instacart_data %>% 
  filter(aisle == "baking ingrediants" | aisle == "dog food care" | aisle == " packeged vegetables fruits") %>% 
  group_by(aisle)
```

4.  *_Make a table showing the mean hour of the day at which Pink Lady
    Apples and Coffee Ice Cream are ordered on each day of the week;
    format this table for human readers (i.e. produce a 2 x 7 table)._*

<!-- end list -->

``` r
# Mean hour of the day each object is purchases on each day of the week
# filter by product, group by products/dow, spread, label dow, rename for table
mean_hour_table = 
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarise(
    mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = product_name, value = mean_hour) %>% 
  mutate(
    order_dow = ordered(order_dow, levels = c(0:6), 
                        labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
                        )) %>% 
  rename("Day of Week" = order_dow)
```

**Average hour of the day products are purchased**

| Day of Week | Coffee Ice Cream | Pink Lady Apples |
| :---------- | ---------------: | ---------------: |
| Sunday      |            13.77 |            13.44 |
| Monday      |            14.32 |            11.36 |
| Tuesday     |            15.38 |            11.70 |
| Wednesday   |            15.32 |            14.25 |
| Thursday    |            15.22 |            11.55 |
| Friday      |            12.26 |            12.78 |
| Saturday    |            13.83 |            11.94 |

## Problem 3

``` r
# import dataset
ny_noaa_data = ny_noaa 
```

Data exploration

Short description, size and structure, key variables, indicating the
extent to which missing data is an issue.

### Questions

1.  *_Do some data cleaning. Create separate variables for year, month,
    and day. Ensure observations for temperature, precipitation, and
    snowfall are given in reasonable units. For snowfall, what are the
    most commonly observed values? Why?_*

2.  *_Make a two-panel plot showing the average temperature in January
    and in July in each station across years. Is there any observable /
    interpretable structure? Any outliers?_*

3.  *_Make a two-panel plot showing (i) tmax vs tmin for the full
    dataset (note that a scatterplot may not be the best option); and
    (ii) make a plot showing the distribution of snowfall values greater
    than 0 and less than 100 separately by year._*
